{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIS Unbalanced dataset - Model training and validation ver one:\n",
    "\n",
    "This model will use data obtained from MODIS (one day only), NDWI, NDWI_ICE, DEM SLOPE and EUCLIDEAN_NORM to predict water and non-water features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Load required libraries, site dependant constants and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "# EOLearn libraries:\n",
    "from eolearn.core import EOTask, EOPatch, LinearWorkflow, LoadTask, SaveTask, FeatureType, EOExecutor\n",
    "from eolearn.core import OverwritePermission\n",
    "\n",
    "# Add to python path parent dictionary\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# load site dependant constants (HERE YOU CAN CHOOSE DIFFERENT LOCATION)\n",
    "from aoi_sites import upe_promice_area as site\n",
    "\n",
    "# load utility functions\n",
    "from utils import io_functions as io_utils\n",
    "from utils import plot_functions as plot_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load sampled eopatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing now following eopatches: ['2974_maxcc_0.05_x-3_y-135', '2975_maxcc_0.05_x-3_y-136', '2976_maxcc_0.05_x-3_y-137', '2977_maxcc_0.05_x-3_y-138', '2978_maxcc_0.05_x-3_y-139', '2979_maxcc_0.05_x-3_y-140', '3062_maxcc_0.05_x-4_y-133', '3063_maxcc_0.05_x-4_y-134', '3064_maxcc_0.05_x-4_y-135', '3065_maxcc_0.05_x-4_y-136', '3066_maxcc_0.05_x-4_y-137', '3067_maxcc_0.05_x-4_y-138', '3068_maxcc_0.05_x-4_y-139', '3069_maxcc_0.05_x-4_y-140', '3174_maxcc_0.05_x-5_y-132', '3175_maxcc_0.05_x-5_y-133', '3176_maxcc_0.05_x-5_y-134', '3177_maxcc_0.05_x-5_y-135', '3178_maxcc_0.05_x-5_y-136', '3179_maxcc_0.05_x-5_y-137', '3180_maxcc_0.05_x-5_y-138', '3181_maxcc_0.05_x-5_y-139', '3182_maxcc_0.05_x-5_y-140', '3306_maxcc_0.05_x-6_y-130', '3307_maxcc_0.05_x-6_y-131', '3308_maxcc_0.05_x-6_y-132', '3309_maxcc_0.05_x-6_y-133', '3310_maxcc_0.05_x-6_y-134', '3311_maxcc_0.05_x-6_y-135', '3312_maxcc_0.05_x-6_y-136', '3313_maxcc_0.05_x-6_y-137', '3314_maxcc_0.05_x-6_y-138', '3315_maxcc_0.05_x-6_y-139', '3316_maxcc_0.05_x-6_y-140', '3469_maxcc_0.05_x-7_y-130', '3470_maxcc_0.05_x-7_y-131', '3471_maxcc_0.05_x-7_y-132', '3472_maxcc_0.05_x-7_y-133', '3473_maxcc_0.05_x-7_y-134', '3474_maxcc_0.05_x-7_y-135', '3475_maxcc_0.05_x-7_y-136', '3476_maxcc_0.05_x-7_y-137', '3477_maxcc_0.05_x-7_y-138', '3478_maxcc_0.05_x-7_y-139', '3479_maxcc_0.05_x-7_y-140']\n"
     ]
    }
   ],
   "source": [
    "eopatches_filepath = '../../data/EOPatches/LANDSAT_8/UPE_PROMICE/UTM_22N/2013_05-2013_10_sampled_50k_modis_new_dataset_ver_1/'\n",
    "\n",
    "eopatches = []\n",
    "list_of_available_patches = io_utils.get_list_of_eopatches(eopatches_filepath)\n",
    "list_in_chunks = io_utils.chunkIt(list_of_available_patches, 1 ) # number of chunks- 1 bc all can go\n",
    "for element in list_in_chunks:\n",
    "    print('Doing now following eopatches:', element )\n",
    "    execution_args = []\n",
    "    for eopatch_name in element:\n",
    "        eopatches.append(EOPatch.load(eopatches_filepath+eopatch_name, lazy_loading=True))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 80 /20 cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eopatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index_list = [i for i in range(0,45)]\n",
    "random.shuffle(index_list)\n",
    "\n",
    "no_of_chunks_to_test = 5\n",
    "train_ID = []\n",
    "test_ID = []\n",
    "for x in range(5):\n",
    "    test_ID.append(index_list.pop())\n",
    "train_ID = index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test indexes: [28, 16, 25, 27, 19]\n",
      "Train indexes: [14, 18, 26, 31, 1, 44, 33, 34, 11, 41, 39, 20, 40, 9, 0, 37, 17, 35, 36, 8, 10, 4, 24, 5, 2, 13, 30, 38, 7, 6, 22, 42, 3, 32, 23, 15, 12, 43, 29, 21]\n"
     ]
    }
   ],
   "source": [
    "print('Test indexes:', test_ID)\n",
    "print('Train indexes:', train_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eopatches = []\n",
    "\n",
    "for i in test_ID:\n",
    "    test_eopatches.append( eopatches.pop(i))\n",
    "train_eopatches = eopatches\n",
    "del eopatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3. Fetch and organise set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the train and test patch IDs\n",
    "\n",
    "# Set the features and the labels for train and test sets\n",
    "features_train = np.array([eopatch.data['DATASET_CLD_200_dil_6_str2_SAMPLED'] for eopatch in train_eopatches if eopatch.data['DATASET_CLD_200_dil_6_str2_SAMPLED'].size > 0])\n",
    "#features_train (number_of_patches,(time,width,height, bands))\n",
    "labels_train = np.array([eopatch.mask['WATER_MASK_ST_025_SAMPLED'] for eopatch in train_eopatches if eopatch.mask['WATER_MASK_ST_025_SAMPLED'].size > 0])\n",
    "#features_train (number_of_patches,(time,width,height, answer=True, False))\n",
    "features_test = np.array([eopatch.data['DATASET_CLD_200_dil_6_str2_SAMPLED'] for eopatch in test_eopatches if eopatch.data['DATASET_CLD_200_dil_6_str2_SAMPLED'].size > 0 ])\n",
    "labels_test = np.array([eopatch.mask['WATER_MASK_ST_025_SAMPLED'] for eopatch in test_eopatches if eopatch.mask['WATER_MASK_ST_025_SAMPLED'].size > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape to  (number_of_patches x time, width, height, bands)\n",
    "features_train_stacked = np.vstack(features_train)\n",
    "labels_train_stacked = np.vstack(labels_train)\n",
    "features_test_stacked = np.vstack(features_test)\n",
    "labels_test_stacked = np.vstack(labels_test)\n",
    "\n",
    "# get shape\n",
    "p_train_x_time, w, h, b = features_train_stacked.shape\n",
    "p_test_x_time, w, h, b = features_test_stacked.shape\n",
    "\n",
    "\n",
    "# reshape to n x m\n",
    "#n - no of observation\n",
    "#m - no of features, - bands in my case, misssing DEM\n",
    "\n",
    "features_train = features_train_stacked.reshape(p_train_x_time * w * h, b)\n",
    "labels_train = labels_train_stacked.reshape(p_train_x_time * w * h, 1)\n",
    "features_test = features_test_stacked.reshape(p_test_x_time * w * h, b)\n",
    "labels_test = labels_test_stacked.reshape(p_test_x_time * w * h, 1)\n",
    "\n",
    "\n",
    "#@TODO\n",
    "# remove points with no reference from training (so we dont train to recognize \"no data\")\n",
    "# interpolation????\n",
    "#mask_train = labels_train == 0\n",
    "#features_train = features_train[~mask_train]\n",
    "#labels_train = labels_train[~mask_train]\n",
    "\n",
    "# remove points with no reference from test (so we dont validate on \"no data\", which doesn't make sense)\n",
    "#mask_test = labels_test == 0\n",
    "#features_test = features_test[~mask_test]\n",
    "#labels_test = labels_test[~mask_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4. Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA:\n",
      "Total observations:  9950000\n",
      "Total water count 4218\n",
      "Percent of water feature among data: 0.00042391959798994974\n",
      "\n",
      "TEST DATA:\n",
      "Total observations:  1550000\n",
      "Total water count 233\n",
      "Percent of water feature among data: 0.00015032258064516128\n"
     ]
    }
   ],
   "source": [
    "#to compare old data distribution (little less )\n",
    "\n",
    "print('TRAIN DATA:')\n",
    "tot_obs = np.size(labels_train)\n",
    "print('Total observations: ', tot_obs )\n",
    "\n",
    "tot_wat = np.count_nonzero(labels_train)\n",
    "print('Total water count', tot_wat )\n",
    "\n",
    "ratio_wat_vs_non_wat = tot_wat/ tot_obs\n",
    "print('Percent of water feature among data:', ratio_wat_vs_non_wat )\n",
    "\n",
    "print('\\nTEST DATA:')\n",
    "\n",
    "tot_obs = np.size(labels_test)\n",
    "print('Total observations: ', tot_obs )\n",
    "\n",
    "tot_wat = np.count_nonzero(labels_test)\n",
    "print('Total water count', tot_wat )\n",
    "\n",
    "ratio_wat_vs_non_wat = tot_wat/ tot_obs\n",
    "print('Percent of water feature among data:', ratio_wat_vs_non_wat )\n",
    "# dataset is half of the previous one!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kamil\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Kamil\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting='gbdt', boosting_type='gbdt', class_weight='balanced',\n",
       "               colsample_bytree=1.0, importance_type='split',\n",
       "               learning_rate=0.01, max_depth=-1, metric='binary_logloss',\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective='binary',\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set up training classes\n",
    "labels_unique = np.unique(labels_train)\n",
    "\n",
    "# Set up the model\n",
    "model = LGBMClassifier(boosting='gbdt',\n",
    "    objective='binary',\n",
    "    learning_rate=0.01,\n",
    "    metric='binary_logloss', #mae: mean absolute error mse: mean squared error\n",
    "    class_weight= 'balanced' \n",
    "                       \n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(features_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./NEW_model_WATER_NONWATER_MODIS_ver_1.0.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment to save the model\n",
    "model_base_name = 'NEW_model_WATER_NONWATER_MODIS_ver_1.0'\n",
    "joblib.dump(model, './{}.pkl'.format(model_base_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  5. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test labels\n",
    "plabels_test = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy 99.2%\n",
      "Classification F1-score 99.6%\n"
     ]
    }
   ],
   "source": [
    "print('Classification accuracy {:.1f}%'.format(100 * metrics.accuracy_score(labels_test, plabels_test)))\n",
    "print('Classification F1-score {:.1f}%'.format(100 * metrics.f1_score(labels_test, plabels_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the train labels\n",
    "plabels_train = model.predict(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy 99.9%\n",
      "Classification F1-score 99.9%\n"
     ]
    }
   ],
   "source": [
    "print('Classification accuracy {:.1f}%'.format(100 * metrics.accuracy_score(labels_train, plabels_train)))\n",
    "print('Classification F1-score {:.1f}%'.format(100 * metrics.f1_score(labels_train, plabels_train, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water Test data size: 233\n",
      "Water True positive: 226 and recall 96.99570815450643 and precision: 0.018676142467564664\n"
     ]
    }
   ],
   "source": [
    "actual_water = np.count_nonzero(labels_test)\n",
    "predicted_water = np.count_nonzero(plabels_test)\n",
    "\n",
    "print('Water Test data size:',actual_water)\n",
    "\n",
    "tp = np.count_nonzero(np.logical_and(labels_test.squeeze(), plabels_test.squeeze()))\n",
    "print('Water True positive:', tp, 'and recall', 100* tp /actual_water, 'and precision:', tp /predicted_water)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BECAUSE THE CLASS ARE NOT WEIGHTED, I HOPE SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['non-water', 'water']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = np.unique(labels_test)\n",
    "print(class_labels)\n",
    "class_names = ['non-water', 'water']\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Class              =  F1  | Recall | Precision\n",
      "         --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kamil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         * non-water            = 99.6 |  99.2  | 100.0\n",
      "         * water                = 3.7 |  97.0  | 1.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f1_scores = metrics.f1_score(labels_test, plabels_test, labels=class_labels, average=None)\n",
    "recall = metrics.recall_score(labels_test, plabels_test, labels=class_labels, average=None)\n",
    "precision = metrics.precision_score(labels_test, plabels_test, labels=class_labels, average=None) \n",
    "\n",
    "print('             Class              =  F1  | Recall | Precision')\n",
    "print('         --------------------------------------------------')\n",
    "for idx, lulctype in enumerate([class_names[idx] for idx in class_labels]):\n",
    "    print('         * {0:20s} = {1:2.1f} |  {2:2.1f}  | {3:2.1f}'.format(lulctype, \n",
    "                                                                         f1_scores[idx] * 100, \n",
    "                                                                         recall[idx] * 100, \n",
    "                                                                         precision[idx] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 6. Plot the standard and transposed Confusion Matrix:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
